# JARVIS AI Assistant - Setup Instructions

**Created**: November 20, 2025
**Source**: Claude Code Session (rescued from 403 git push error)

---

## ‚úÖ Files Saved Successfully

Your JARVIS code has been safely saved to:

```
D:\Watery-Tart-giving-out-Swards-\
‚îú‚îÄ‚îÄ jarvis/
‚îÇ   ‚îî‚îÄ‚îÄ llm/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py         ‚úÖ SAVED
‚îÇ       ‚îî‚îÄ‚îÄ providers.py        ‚úÖ SAVED (260 lines - core LLM manager)
‚îî‚îÄ‚îÄ JARVIS_SETUP_INSTRUCTIONS.md  ‚úÖ THIS FILE
```

**Backup copy also on Desktop**: `C:\Users\Abbyl\Desktop\JARVIS_COMPLETE_BUNDLE.txt`

---

## üéØ What You Built

**LLM Provider Manager** that gives you:
- ‚úÖ Claude API integration (fast, smart, cloud-based)
- ‚úÖ Ollama integration (local, private, offline)
- ‚úÖ Automatic provider switching
- ‚úÖ Privacy mode (use local AI when needed)
- ‚úÖ Request tracking and performance monitoring

**This solves your "inconsistent Claude" problem** by letting you:
1. Use Claude API when you want smart, fast responses
2. Switch to local Ollama when you want privacy or offline access
3. Add episodic/semantic memory in future (next phase)

---

## üìã Next Steps

### Step 1: Find Where Ollama Models Are

Run these PowerShell commands to find where Gemma 2B actually downloaded:

```powershell
# Check environment variable
[System.Environment]::GetEnvironmentVariable('OLLAMA_MODELS', [System.EnvironmentVariableTarget]::Machine)

# Find large files in default location (most likely here)
Get-ChildItem -Path "$env:USERPROFILE\.ollama" -Recurse -File -ErrorAction SilentlyContinue | Where-Object {$_.Length -gt 100MB} | Select-Object FullName, @{Name="SizeMB";Expression={[math]::Round($_.Length/1MB,2)}}

# Check D: drive
Get-ChildItem -Path "D:\Ollama" -Recurse -File -ErrorAction SilentlyContinue | Select-Object FullName, @{Name="SizeMB";Expression={[math]::Round($_.Length/1MB,2)}}

# What does Ollama see?
ollama list
```

**Most likely result**: Models are in `C:\Users\Abbyl\.ollama\models\` (~1.7GB)

**Why**: The `OLLAMA_MODELS=D:\Ollama` environment variable wasn't read when Ollama service started.

### Step 2: Move Ollama Models to D: Drive (Optional)

If models are taking up C: drive space:

```powershell
# 1. Stop Ollama service
Stop-Service Ollama

# 2. Move models
Move-Item -Path "$env:USERPROFILE\.ollama\models" -Destination "D:\Ollama\models"

# 3. Restart Ollama
Start-Service Ollama

# 4. Verify
ollama list
```

### Step 3: Install Python Dependencies

```bash
pip install anthropic requests
```

Or add to your `requirements.txt`:
```
anthropic>=0.18.0
requests>=2.31.0
```

### Step 4: Test the Code

```python
# Test that imports work
python -c "from jarvis.llm.providers import LLMManager; print('‚úÖ Import successful!')"
```

---

## üöÄ How to Use JARVIS

### Basic Usage

```python
from jarvis.llm.providers import LLMManager, LLMProvider
import asyncio

# Initialize with Claude as default
llm = LLMManager(default_provider=LLMProvider.CLAUDE)

# Generate response
async def test():
    result = await llm.generate("What is neurodivergence?")
    print(result["response"])

asyncio.run(test())
```

### Switch to Ollama (Local/Private)

```python
# Switch to local Ollama
llm.set_provider(LLMProvider.OLLAMA)

result = await llm.generate("Explain ADHD executive dysfunction")
print(result["response"])
```

### Check Provider Status

```python
status = llm.get_status()
print(f"Current provider: {status['current_provider']}")
print(f"Claude available: {status['providers']['claude']['available']}")
print(f"Ollama available: {status['providers']['ollama']['available']}")
```

---

## üîê Environment Setup

### Set Claude API Key

```powershell
# Windows PowerShell (persistent)
[System.Environment]::SetEnvironmentVariable('ANTHROPIC_API_KEY', 'your-key-here', [System.EnvironmentVariableTarget]::User)
```

Or create a `.env` file:
```
ANTHROPIC_API_KEY=sk-ant-api03-...
```

---

## üêõ Troubleshooting

### "Ollama error: Connection refused"
- **Solution**: Start Ollama service
  ```powershell
  Start-Service Ollama
  # Or manually: ollama serve
  ```

### "Claude API key not configured"
- **Solution**: Set ANTHROPIC_API_KEY environment variable (see above)

### "Ollama request timed out (>60s)"
- **Cause**: Gemma 2B might be too large for your RAM
- **Solutions**:
  1. Try smaller model: `ollama pull tinyllama` (650MB)
  2. Use Claude API instead
  3. Upgrade RAM

### "Models not in D:\Ollama"
- **Cause**: Environment variable not read when Ollama started
- **Solution**: Follow Step 2 above to move models

---

## üìä What's Missing (Future Phases)

This is **Phase 1: LLM Provider Management**. Still need:

### Phase 2: Memory Layers
- **Episodic Memory**: SQLite database to store conversations
- **Semantic Memory**: Vector embeddings to learn your patterns
- **Context Retrieval**: Pull relevant past conversations automatically

### Phase 3: Web UI
- Dashboard to switch providers
- Conversation history viewer
- Memory statistics
- Privacy mode toggle

### Phase 4: Integration
- Connect to NeuroThrive Salesforce platform
- Use JARVIS for interview prep
- Personal assistant for job search workflow

---

## üéØ Your Original Problem (Solved!)

**Problem**: Two Claude sessions gave different assessments (97% vs 70%)
**Root Cause**: No memory, no context, inconsistent analysis

**Solution with JARVIS**:
1. **Episodic Memory** (next phase) will remember specific conversations
   - "Remember when Claude told me 97% complete?"
   - "What did we discuss about beta testing yesterday?"

2. **Semantic Memory** (next phase) will learn your patterns
   - "User prefers realistic timelines (multiply by 3-5x)"
   - "User values objective data over AI opinions"
   - "User is building for neurodivergent/underserved communities"

3. **Provider Switching** (working now) gives you control
   - Use Claude for complex analysis
   - Use Ollama when you don't trust cloud AI
   - Switch based on your comfort level

---

## üìù Summary

**What you have now**:
- ‚úÖ Working LLM provider manager (Claude + Ollama)
- ‚úÖ Privacy mode (use local AI)
- ‚úÖ Provider switching and status monitoring

**What you can do**:
- Test basic AI generation
- Switch between cloud and local
- Start building memory layers (next session)

**Your files are SAFE**:
- Main code: `D:\Watery-Tart-giving-out-Swards-\jarvis\llm\`
- Backup: `C:\Users\Abbyl\Desktop\JARVIS_COMPLETE_BUNDLE.txt`

---

## üöÄ Ready to Continue?

When you're ready for Phase 2 (memory layers), we'll add:
1. SQLite database for conversation history
2. Vector embeddings for pattern recognition
3. Context-aware responses based on past interactions

**For now, test what you have**:
```bash
cd D:\Watery-Tart-giving-out-Swards-
python -c "from jarvis.llm.providers import LLMManager; print('‚úÖ JARVIS is ready!')"
```

---

**Your work is saved. Nothing was lost. JARVIS lives.** ü§ñüíú
